{
	"AdminClient":[
	  {
	    "id": "ssl.key.password",
	    "description": "The password of the private key in the key store file orthe PEM key specified in `ssl.keystore.key'. This is required for clients only if two-way authentication is configured.",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.keystore.certificate.chain",
	    "description": "Certificate chain in the format specified by 'ssl.keystore.type'. Default SSL engine factory supports only PEM format with a list of X.509 certificates",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.keystore.key",
	    "description": "Private key in the format specified by 'ssl.keystore.type'. Default SSL engine factory supports only PEM format with PKCS#8 keys. If the key is encrypted, key password must be specified using 'ssl.key.password'",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.keystore.location",
	    "description": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.keystore.password",
	    "description": "The store password for the key store file. This is optional for client and only needed if 'ssl.keystore.location' is configured. Key store password is not supported for PEM format.",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.truststore.certificates",
	    "description": "Trusted certificates in the format specified by 'ssl.truststore.type'. Default SSL engine factory supports only PEM format with X.509 certificates.",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.truststore.location",
	    "description": "The location of the trust store file.",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.truststore.password",
	    "description": "The password for the trust store file. If a password is not set, trust store file configured will still be used, but integrity checking is disabled. Trust store password is not supported for PEM format.",
	    "defaultValue": ""
	  },
	  {
	    "id": "client.dns.lookup",
	    "description": "Controls how the client uses DNS lookups. If set to use_all_dns_ips, connect to each returned IP address in sequence until a successful connection is established. After a disconnection, the next IP is used. Once all IPs have been used once, the client resolves the IP(s) from the hostname again (both the JVM and the OS cache DNS name lookups, however). If set to resolve_canonical_bootstrap_servers_only, resolve each bootstrap address into a list of canonical names. After the bootstrap phase, this behaves the same as use_all_dns_ips.",
	    "defaultValue": "use_all_dns_ips"
	  },
	  {
	    "id": "client.id",
	    "description": "An id string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included in server-side request logging.",
	    "defaultValue": ""
	  },
	  {
	    "id": "connections.max.idle.ms",
	    "description": "Close idle connections after the number of milliseconds specified by this config.",
	    "defaultValue": "300000"
	  },
	  {
	    "id": "default.api.timeout.ms",
	    "description": "Specifies the timeout (in milliseconds) for client APIs. This configuration is used as the default timeout for all client operations that do not specify a timeout parameter.",
	    "defaultValue": "60000"
	  },
	  {
	    "id": "receive.buffer.bytes",
	    "description": "The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used.",
	    "defaultValue": "65536"
	  },
	  {
	    "id": "request.timeout.ms",
	    "description": "The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted.",
	    "defaultValue": "30000"
	  },
	  {
	    "id": "sasl.client.callback.handler.class",
	    "description": "The fully qualified name of a SASL client callback handler class that implements the AuthenticateCallbackHandler interface.",
	    "defaultValue": ""
	  },
	  {
	    "id": "sasl.jaas.config",
	    "description": "JAAS login context parameters for SASL connections in the format used by JAAS configuration files. JAAS configuration file format is described here. The format for the value is: loginModuleClass controlFlag (optionName=optionValue)*;. For brokers, the config must be prefixed with listener prefix and SASL mechanism name in lower-case. For example, listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=com.example.ScramLoginModule required;",
	    "defaultValue": ""
	  },
	  {
	    "id": "sasl.kerberos.service.name",
	    "description": "The Kerberos principal name that Kafka runs as. This can be defined either in Kafka's JAAS config or in Kafka's config.",
	    "defaultValue": ""
	  },
	  {
	    "id": "sasl.login.callback.handler.class",
	    "description": "The fully qualified name of a SASL login callback handler class that implements the AuthenticateCallbackHandler interface. For brokers, login callback handler config must be prefixed with listener prefix and SASL mechanism name in lower-case. For example, listener.name.sasl_ssl.scram-sha-256.sasl.login.callback.handler.class=com.example.CustomScramLoginCallbackHandler",
	    "defaultValue": ""
	  },
	  {
	    "id": "sasl.login.class",
	    "description": "The fully qualified name of a class that implements the Login interface. For brokers, login config must be prefixed with listener prefix and SASL mechanism name in lower-case. For example, listener.name.sasl_ssl.scram-sha-256.sasl.login.class=com.example.CustomScramLogin",
	    "defaultValue": ""
	  },
	  {
	    "id": "sasl.mechanism",
	    "description": "SASL mechanism used for client connections. This may be any mechanism for which a security provider is available. GSSAPI is the default mechanism.",
	    "defaultValue": "GSSAPI"
	  },
	  {
	    "id": "security.protocol",
	    "description": "Protocol used to communicate with brokers. ",
	    "defaultValue": "PLAINTEXT"
	  },
	  {
	    "id": "send.buffer.bytes",
	    "description": "The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used.",
	    "defaultValue": "131072"
	  },
	  {
	    "id": "socket.connection.setup.timeout.max.ms",
	    "description": "The maximum amount of time the client will wait for the socket connection to be established. The connection setup timeout will increase exponentially for each consecutive connection failure up to this maximum. To avoid connection storms, a randomization factor of 0.2 will be applied to the timeout resulting in a random range between 20% below and 20% above the computed value.",
	    "defaultValue": "30000"
	  },
	  {
	    "id": "socket.connection.setup.timeout.ms",
	    "description": "The amount of time the client will wait for the socket connection to be established. If the connection is not built before the timeout elapses, clients will close the socket channel.",
	    "defaultValue": "10000"
	  },
	  {
	    "id": "ssl.enabled.protocols",
	    "description": "The list of protocols enabled for SSL connections. The default is 'TLSv1.2,TLSv1.3' when running with Java 11 or newer, 'TLSv1.2' otherwise. With the default value for Java 11, clients and servers will prefer TLSv1.3 if both support it and fallback to TLSv1.2 otherwise (assuming both support at least TLSv1.2). This default should be fine for most cases. Also see the config documentation for `ssl.protocol`.",
	    "defaultValue": "TLSv1.2"
	  },
	  {
	    "id": "ssl.keystore.type",
	    "description": "The file format of the key store file. This is optional for client.",
	    "defaultValue": "JKS"
	  },
	  {
	    "id": "ssl.protocol",
	    "description": "The SSL protocol used to generate the SSLContext. The default is 'TLSv1.3' when running with Java 11 or newer, 'TLSv1.2' otherwise. This value should be fine for most use cases. Allowed values in recent JVMs are 'TLSv1.2' and 'TLSv1.3'. 'TLS', 'TLSv1.1', 'SSL', 'SSLv2' and 'SSLv3' may be supported in older JVMs, but their usage is discouraged due to known security vulnerabilities. With the default value for this config and 'ssl.enabled.protocols', clients will downgrade to 'TLSv1.2' if the server does not support 'TLSv1.3'. If this config is set to 'TLSv1.2', clients will not use 'TLSv1.3' even if it is one of the values in ssl.enabled.protocols and the server only supports 'TLSv1.3'.",
	    "defaultValue": "TLSv1.2"
	  },
	  {
	    "id": "ssl.provider",
	    "description": "The name of the security provider used for SSL connections. Default value is the default security provider of the JVM.",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.truststore.type",
	    "description": "The file format of the trust store file.",
	    "defaultValue": "JKS"
	  },
	  {
	    "id": "host.resolver.class",
	    "description": "Class to use for resolving a host name",
	    "defaultValue": "org.apache.kafka.clients.DefaultHostResolver"
	  },
	  {
	    "id": "metadata.max.age.ms",
	    "description": "The period of time in milliseconds after which we force a refresh of metadata even if we haven't seen any partition leadership changes to proactively discover any new brokers or partitions.",
	    "defaultValue": "300000"
	  },
	  {
	    "id": "metric.reporters",
	    "description": "A list of classes to use as metrics reporters. Implementing the org.apache.kafka.common.metrics.MetricsReporter interface allows plugging in classes that will be notified of new metric creation. The JmxReporter is always included to register JMX statistics.",
	    "defaultValue": ""
	  },
	  {
	    "id": "metrics.num.samples",
	    "description": "The number of samples maintained to compute metrics.",
	    "defaultValue": "2"
	  },
	  {
	    "id": "metrics.recording.level",
	    "description": "The highest recording level for metrics.",
	    "defaultValue": "INFO"
	  },
	  {
	    "id": "metrics.sample.window.ms",
	    "description": "The window of time a metrics sample is computed over.",
	    "defaultValue": "30000"
	  },
	  {
	    "id": "reconnect.backoff.max.ms",
	    "description": "The maximum amount of time in milliseconds to wait when reconnecting to a broker that has repeatedly failed to connect. If provided, the backoff per host will increase exponentially for each consecutive connection failure, up to this maximum. After calculating the backoff increase, 20% random jitter is added to avoid connection storms.",
	    "defaultValue": "1000"
	  },
	  {
	    "id": "reconnect.backoff.ms",
	    "description": "The base amount of time to wait before attempting to reconnect to a given host. This avoids repeatedly connecting to a host in a tight loop. This backoff applies to all connection attempts by the client to a broker.",
	    "defaultValue": "50"
	  },
	  {
	    "id": "retries",
	    "description": "Setting a value greater than zero will cause the client to resend any request that fails with a potentially transient error. It is recommended to set the value to either zero or `MAX_VALUE` and use corresponding timeout parameters to control how long a client should retry a request.",
	    "defaultValue": "2147483647"
	  },
	  {
	    "id": "retry.backoff.ms",
	    "description": "The amount of time to wait before attempting to retry a failed request. This avoids repeatedly sending requests in a tight loop under some failure scenarios.",
	    "defaultValue": "100"
	  },
	  {
	    "id": "sasl.kerberos.kinit.cmd",
	    "description": "Kerberos kinit command path.",
	    "defaultValue": "/usr/bin/kinit"
	  },
	  {
	    "id": "sasl.kerberos.min.time.before.relogin",
	    "description": "Login thread sleep time between refresh attempts.",
	    "defaultValue": "60000"
	  },
	  {
	    "id": "sasl.kerberos.ticket.renew.jitter",
	    "description": "Percentage of random jitter added to the renewal time.",
	    "defaultValue": "0.05"
	  },
	  {
	    "id": "sasl.kerberos.ticket.renew.window.factor",
	    "description": "Login thread will sleep until the specified window factor of time from last refresh to ticket's expiry has been reached, at which time it will try to renew the ticket.",
	    "defaultValue": "0.8"
	  },
	  {
	    "id": "sasl.login.refresh.buffer.seconds",
	    "description": "The amount of buffer time before credential expiration to maintain when refreshing a credential, in seconds. If a refresh would otherwise occur closer to expiration than the number of buffer seconds then the refresh will be moved up to maintain as much of the buffer time as possible. Legal values are between 0 and 3600 (1 hour); a default value of 300 (5 minutes) is used if no value is specified. This value and sasl.login.refresh.min.period.seconds are both ignored if their sum exceeds the remaining lifetime of a credential. Currently applies only to OAUTHBEARER.",
	    "defaultValue": "300"
	  },
	  {
	    "id": "sasl.login.refresh.min.period.seconds",
	    "description": "The desired minimum time for the login refresh thread to wait before refreshing a credential, in seconds. Legal values are between 0 and 900 (15 minutes); a default value of 60 (1 minute) is used if no value is specified. This value and sasl.login.refresh.buffer.seconds are both ignored if their sum exceeds the remaining lifetime of a credential. Currently applies only to OAUTHBEARER.",
	    "defaultValue": "60"
	  },
	  {
	    "id": "sasl.login.refresh.window.factor",
	    "description": "Login refresh thread will sleep until the specified window factor relative to the credential's lifetime has been reached, at which time it will try to refresh the credential. Legal values are between 0.5 (50%) and 1.0 (100%) inclusive; a default value of 0.8 (80%) is used if no value is specified. Currently applies only to OAUTHBEARER.",
	    "defaultValue": "0.8"
	  },
	  {
	    "id": "sasl.login.refresh.window.jitter",
	    "description": "The maximum amount of random jitter relative to the credential's lifetime that is added to the login refresh thread's sleep time. Legal values are between 0 and 0.25 (25%) inclusive; a default value of 0.05 (5%) is used if no value is specified. Currently applies only to OAUTHBEARER.",
	    "defaultValue": "0.05"
	  },
	  {
	    "id": "security.providers",
	    "description": "A list of configurable creator classes each returning a provider implementing security algorithms. These classes should implement the org.apache.kafka.common.security.auth.SecurityProviderCreator interface.",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.cipher.suites",
	    "description": "A list of cipher suites. This is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. By default all the available cipher suites are supported.",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.endpoint.identification.algorithm",
	    "description": "The endpoint identification algorithm to validate server hostname using server certificate.",
	    "defaultValue": "https"
	  },
	  {
	    "id": "ssl.engine.factory.class",
	    "description": "The class of type org.apache.kafka.common.security.auth.SslEngineFactory to provide SSLEngine objects. Default value is org.apache.kafka.common.security.ssl.DefaultSslEngineFactory",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.keymanager.algorithm",
	    "description": "The algorithm used by key manager factory for SSL connections. Default value is the key manager factory algorithm configured for the Java Virtual Machine.",
	    "defaultValue": "SunX509"
	  },
	  {
	    "id": "ssl.secure.random.implementation",
	    "description": "The SecureRandom PRNG implementation to use for SSL cryptography operations.",
	    "defaultValue": ""
	  },
	  {
	    "id": "ssl.trustmanager.algorithm",
	    "description": "The algorithm used by trust manager factory for SSL connections. Default value is the trust manager factory algorithm configured for the Java Virtual Machine.",
	    "defaultValue": "PKIX"
	  }
	],
	"topics":[
	  {
	    "id": "cleanup.policy",
	    "description": "A string that is either 'delete' or 'compact' or both. This string designates the retention policy to use on old log segments. The default policy ('delete') will discard old segments when their retention time or size limit has been reached. The 'compact' setting will enable log compaction on the topic.",
	    "defaultValue": "delete"
	  },
	  {
	    "id": "compression.type",
	    "description": "Specify the final compression type for a given topic. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'uncompressed' which is equivalent to no compression; and 'producer' which means retain the original compression codec set by the producer.",
	    "defaultValue": "producer"
	  },
	  {
	    "id": "delete.retention.ms",
	    "description": "The amount of time to retain delete tombstone markers for log compacted topics. This setting also gives a bound on the time in which a consumer must complete a read if they begin from offset 0 to ensure that they get a valid snapshot of the final stage (otherwise delete tombstones may be collected before they complete their scan).",
	    "defaultValue": "86400000"
	  },
	  {
	    "id": "file.delete.delay.ms",
	    "description": "The time to wait before deleting a file from the filesystem",
	    "defaultValue": "60000"
	  },
	  {
	    "id": "flush.messages",
	    "description": "This setting allows specifying an interval at which we will force an fsync of data written to the log. For example if this was set to 1 we would fsync after every message; if it were 5 we would fsync after every five messages. In general we recommend you not set this and use replication for durability and allow the operating system's background flush capabilities as it is more efficient. This setting can be overridden on a per-topic basis (see the per-topic configuration section).",
	    "defaultValue": "9223372036854775807"
	  },
	  {
	    "id": "flush.ms",
	    "description": "This setting allows specifying a time interval at which we will force an fsync of data written to the log. For example if this was set to 1000 we would fsync after 1000 ms had passed. In general we recommend you not set this and use replication for durability and allow the operating system's background flush capabilities as it is more efficient.",
	    "defaultValue": "9223372036854775807"
	  },
	  {
	    "id": "follower.replication.throttled.replicas",
	    "description": "A list of replicas for which log replication should be throttled on the follower side. The list should describe a set of replicas in the form [PartitionId]:[BrokerId],[PartitionId]:[BrokerId]:... or alternatively the wildcard '*' can be used to throttle all replicas for this topic.",
	    "defaultValue": ""
	  },
	  {
	    "id": "index.interval.bytes",
	    "description": "This setting controls how frequently Kafka adds an index entry to its offset index. The default setting ensures that we index a message roughly every 4096 bytes. More indexing allows reads to jump closer to the exact position in the log but makes the index larger. You probably don't need to change this.",
	    "defaultValue": "4096"
	  },
	  {
	    "id": "leader.replication.throttled.replicas",
	    "description": "A list of replicas for which log replication should be throttled on the leader side. The list should describe a set of replicas in the form [PartitionId]:[BrokerId],[PartitionId]:[BrokerId]:... or alternatively the wildcard '*' can be used to throttle all replicas for this topic.",
	    "defaultValue": ""
	  },
	  {
	    "id": "max.compaction.lag.ms",
	    "description": "The maximum time a message will remain ineligible for compaction in the log. Only applicable for logs that are being compacted.",
	    "defaultValue": "9223372036854775807"
	  },
	  {
	    "id": "max.message.bytes",
	    "description": "The largest record batch size allowed by Kafka (after compression if compression is enabled). If this is increased and there are consumers older than 0.10.2, the consumers' fetch size must also be increased so that they can fetch record batches this large. In the latest message format version, records are always grouped into batches for efficiency. In previous message format versions, uncompressed records are not grouped into batches and this limit only applies to a single record in that case.",
	    "defaultValue": "1048588"
	  },
	  {
	    "id": "message.format.version",
	    "description": "[DEPRECATED] Specify the message format version the broker will use to append messages to the logs. The value of this config is always assumed to be `3.0` if `inter.broker.protocol.version` is 3.0 or higher (the actual config value is ignored). Otherwise, the value should be a valid ApiVersion. Some examples are: 0.10.0, 1.1, 2.8, 3.0. By setting a particular message format version, the user is certifying that all the existing messages on disk are smaller or equal than the specified version. Setting this value incorrectly will cause consumers with older versions to break as they will receive messages with a format that they don't understand.",
	    "defaultValue": "3.0-IV1"
	  },
	  {
	    "id": "message.timestamp.difference.max.ms",
	    "description": "The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message. If message.timestamp.type=CreateTime, a message will be rejected if the difference in timestamp exceeds this threshold. This configuration is ignored if message.timestamp.type=LogAppendTime.",
	    "defaultValue": "9223372036854775807"
	  },
	  {
	    "id": "message.timestamp.type",
	    "description": "Define whether the timestamp in the message is message create time or log append time. The value should be either `CreateTime` or `LogAppendTime`",
	    "defaultValue": "CreateTime"
	  },
	  {
	    "id": "min.cleanable.dirty.ratio",
	    "description": "This configuration controls how frequently the log compactor will attempt to clean the log (assuming log compaction is enabled). By default we will avoid cleaning a log where more than 50% of the log has been compacted. This ratio bounds the maximum space wasted in the log by duplicates (at 50% at most 50% of the log could be duplicates). A higher ratio will mean fewer, more efficient cleanings but will mean more wasted space in the log. If the max.compaction.lag.ms or the min.compaction.lag.ms configurations are also specified, then the log compactor considers the log to be eligible for compaction as soon as either: (i) the dirty ratio threshold has been met and the log has had dirty (uncompacted) records for at least the min.compaction.lag.ms duration, or (ii) if the log has had dirty (uncompacted) records for at most the max.compaction.lag.ms period.",
	    "defaultValue": "0.5"
	  },
	  {
	    "id": "min.compaction.lag.ms",
	    "description": "The minimum time a message will remain uncompacted in the log. Only applicable for logs that are being compacted.",
	    "defaultValue": "0"
	  },
	  {
	    "id": "min.insync.replicas",
	    "description": "When a producer sets acks to 'all' (or '-1'), this configuration specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful. If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or NotEnoughReplicasAfterAppend). When used together, min.insync.replicas and acks allow you to enforce greater durability guarantees. A typical scenario would be to create a topic with a replication factor of 3, set min.insync.replicas to 2, and produce with acks of 'all'. This will ensure that the producer raises an exception if a majority of replicas do not receive a write.",
	    "defaultValue": "1"
	  },
	  {
	    "id": "preallocate",
	    "description": "True if we should preallocate the file on disk when creating a new log segment.",
	    "defaultValue": "false"
	  },
	  {
	    "id": "retention.bytes",
	    "description": "This configuration controls the maximum size a partition (which consists of log segments) can grow to before we will discard old log segments to free up space if we are using the 'delete' retention policy. By default there is no size limit only a time limit. Since this limit is enforced at the partition level, multiply it by the number of partitions to compute the topic retention in bytes.",
	    "defaultValue": "-1"
	  },
	  {
	    "id": "retention.ms",
	    "description": "This configuration controls the maximum time we will retain a log before we will discard old log segments to free up space if we are using the 'delete' retention policy. This represents an SLA on how soon consumers must read their data. If set to -1, no time limit is applied.",
	    "defaultValue": "604800000"
	  },
	  {
	    "id": "segment.bytes",
	    "description": "This configuration controls the segment file size for the log. Retention and cleaning is always done a file at a time so a larger segment size means fewer files but less granular control over retention.",
	    "defaultValue": "1073741824"
	  },
	  {
	    "id": "segment.index.bytes",
	    "description": "This configuration controls the size of the index that maps offsets to file positions. We preallocate this index file and shrink it only after log rolls. You generally should not need to change this setting.",
	    "defaultValue": "10485760"
	  },
	  {
	    "id": "segment.jitter.ms",
	    "description": "The maximum random jitter subtracted from the scheduled segment roll time to avoid thundering herds of segment rolling",
	    "defaultValue": "0"
	  },
	  {
	    "id": "segment.ms",
	    "description": "This configuration controls the period of time after which Kafka will force the log to roll even if the segment file isn't full to ensure that retention can delete or compact old data.",
	    "defaultValue": "604800000"
	  },
	  {
	    "id": "unclean.leader.election.enable",
	    "description": "Indicates whether to enable replicas not in the ISR set to be elected as leader as a last resort, even though doing so may result in data loss.",
	    "defaultValue": "false"
	  },
	  {
	    "id": "message.downconversion.enable",
	    "description": "This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests. When set to false, broker will not perform down-conversion for consumers expecting an older message format. The broker responds with UNSUPPORTED_VERSION error for consume requests from such older clients. This configurationdoes not apply to any message format conversion that might be required for replication to followers.",
	    "defaultValue": "true"
	  }
	]
}